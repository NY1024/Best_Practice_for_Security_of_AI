{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n",
    "\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if tf.executing_eagerly():\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from art.attacks.poisoning import PoisoningAttackBackdoor\n",
    "from art.attacks.poisoning.perturbations import add_pattern_bd, add_single_bd, insert_image\n",
    "from art.estimators.classification import KerasClassifier\n",
    "from art.utils import load_mnist, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\"[%(levelname)s] %(message)s\")\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_raw, y_raw), (x_raw_test, y_raw_test), min_, max_ = load_mnist(raw=True)\n",
    "\n",
    "n_train = np.shape(x_raw)[0]\n",
    "num_selection = 7500\n",
    "random_selection_indices = np.random.choice(n_train, num_selection)\n",
    "x_raw = x_raw[random_selection_indices]\n",
    "y_raw = y_raw[random_selection_indices]\n",
    "\n",
    "BACKDOOR_TYPE = \"pattern\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_val = np.max(x_raw)\n",
    "def add_modification(x):\n",
    "    if BACKDOOR_TYPE == 'pattern':\n",
    "        return add_pattern_bd(x, pixel_value=max_val)\n",
    "    elif BACKDOOR_TYPE == 'pixel':\n",
    "        return add_single_bd(x, pixel_value=max_val) \n",
    "    else:\n",
    "        raise(\"Unknown backdoor type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_dataset(x_clean, y_clean, percent_poison, poison_func):\n",
    "    x_poison = np.copy(x_clean)\n",
    "    y_poison = np.copy(y_clean)\n",
    "    is_poison = np.zeros(np.shape(y_poison))\n",
    "    \n",
    "    sources = np.arange(10)\n",
    "    targets = (np.arange(10) + 1) % 10 \n",
    "    for i, (src, tgt) in enumerate(zip(sources, targets)):\n",
    "        n_points_in_tgt = np.size(np.where(y_clean == tgt))\n",
    "        num_poison = round((percent_poison * n_points_in_tgt) / (1 - percent_poison))\n",
    "        src_imgs = x_clean[y_clean == src]\n",
    "\n",
    "        n_points_in_src = np.shape(src_imgs)[0]\n",
    "        indices_to_be_poisoned = np.random.choice(n_points_in_src, num_poison)\n",
    "\n",
    "        imgs_to_be_poisoned = np.copy(src_imgs[indices_to_be_poisoned])\n",
    "        backdoor_attack = PoisoningAttackBackdoor(poison_func)\n",
    "        imgs_to_be_poisoned, poison_labels = backdoor_attack.poison(imgs_to_be_poisoned, y=np.ones(num_poison) * tgt)\n",
    "        x_poison = np.append(x_poison, imgs_to_be_poisoned, axis=0)\n",
    "        y_poison = np.append(y_poison, poison_labels, axis=0)\n",
    "        is_poison = np.append(is_poison, np.ones(num_poison))\n",
    "\n",
    "    is_poison = is_poison != 0\n",
    "\n",
    "    return is_poison, x_poison, y_poison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_poison = 0.33\n",
    "(is_poison_train, x_poisoned_raw, y_poisoned_raw) = poison_dataset(x_raw, y_raw, percent_poison, add_modification)\n",
    "x_train, y_train = preprocess(x_poisoned_raw, y_poisoned_raw)\n",
    "\n",
    "x_train = np.expand_dims(x_train, axis=3)\n",
    "\n",
    "\n",
    "(is_poison_test, x_poisoned_raw_test, y_poisoned_raw_test) = poison_dataset(x_raw_test, y_raw_test, percent_poison, add_modification)\n",
    "x_test, y_test = preprocess(x_poisoned_raw_test, y_poisoned_raw_test)\n",
    "\n",
    "x_test = np.expand_dims(x_test, axis=3)\n",
    "\n",
    "n_train = np.shape(y_train)[0]\n",
    "shuffled_indices = np.arange(n_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "x_train = x_train[shuffled_indices]\n",
    "y_train = y_train[shuffled_indices]\n",
    "is_poison_train = is_poison_train[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.keras:Inferred 7 hidden layers on Keras classifier.\n",
      "[INFO] Inferred 7 hidden layers on Keras classifier.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11194 samples\n",
      "Epoch 1/5\n",
      "11194/11194 [==============================] - 11s 1ms/sample - loss: 1.2477 - accuracy: 0.5455\n",
      "Epoch 2/5\n",
      "11194/11194 [==============================] - 11s 1ms/sample - loss: 0.4192 - accuracy: 0.8697\n",
      "Epoch 3/5\n",
      "11194/11194 [==============================] - 11s 1ms/sample - loss: 0.2497 - accuracy: 0.9254\n",
      "Epoch 4/5\n",
      "11194/11194 [==============================] - 11s 994us/sample - loss: 0.1913 - accuracy: 0.9411\n",
      "Epoch 5/5\n",
      "11194/11194 [==============================] - 11s 989us/sample - loss: 0.1518 - accuracy: 0.9535\n"
     ]
    }
   ],
   "source": [
    "classifier = KerasClassifier(model=model, clip_values=(0, 1))\n",
    "classifier.fit(x_train, y_train, nb_epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/envs/exp2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/envs/exp2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "[WARNING] From /root/anaconda3/envs/exp2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clean test set accuracy: 97.54%\n"
     ]
    }
   ],
   "source": [
    "clean_x_test = x_test[is_poison_test == 0]\n",
    "clean_y_test = y_test[is_poison_test == 0]\n",
    "\n",
    "clean_preds = np.argmax(classifier.predict(clean_x_test), axis=1)\n",
    "clean_correct = np.sum(clean_preds == np.argmax(clean_y_test, axis=1))\n",
    "clean_total = clean_y_test.shape[0]\n",
    "\n",
    "clean_acc = clean_correct / clean_total\n",
    "print(\"\\nClean test set accuracy: %.2f%%\" % (clean_acc * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Effectiveness of poison: 95.96%\n"
     ]
    }
   ],
   "source": [
    "poison_x_test = x_test[is_poison_test]\n",
    "poison_y_test = y_test[is_poison_test]\n",
    "\n",
    "poison_preds = np.argmax(classifier.predict(poison_x_test), axis=1)\n",
    "poison_correct = np.sum(poison_preds == np.argmax(poison_y_test, axis=1))\n",
    "poison_total = poison_y_test.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "poison_acc = poison_correct / poison_total\n",
    "print(\"\\n Effectiveness of poison: %.2f%%\" % (poison_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:art.estimators.classification.keras:Inferred 7 hidden layers on Keras classifier.\n",
      "[INFO] Inferred 7 hidden layers on Keras classifier.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "classifier = KerasClassifier(model=model, clip_values=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.defences.preprocessor.preprocessor import Preprocessor\n",
    "from art.utils import to_categorical\n",
    "from typing import Optional, Tuple\n",
    "class Mixup(Preprocessor):\n",
    "\n",
    "\n",
    "    params = [\"num_classes\", \"alpha\", \"num_mix\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        alpha: float = 1.0,\n",
    "        num_mix: int = 2,\n",
    "        apply_fit: bool = True,\n",
    "        apply_predict: bool = False,\n",
    "    ) -> None:\n",
    "\n",
    "        super().__init__(is_fitted=True, apply_fit=apply_fit, apply_predict=apply_predict)\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.num_mix = num_mix\n",
    "        self._check_params()\n",
    "\n",
    "    def __call__(self, x: np.ndarray, y: Optional[np.ndarray] = None) -> Tuple[np.ndarray, Optional[np.ndarray]]:\n",
    "\n",
    "        if y is None:\n",
    "            raise ValueError(\"Labels `y` cannot be None.\")\n",
    "\n",
    "        \n",
    "        if len(y.shape) == 2:\n",
    "            y_one_hot = y\n",
    "        elif len(y.shape) == 1:\n",
    "            y_one_hot = to_categorical(y, self.num_classes)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Shape of labels not recognised. \"\n",
    "                \"Please provide labels in shape (nb_samples,) or (nb_samples, nb_classes)\"\n",
    "            )\n",
    "\n",
    "        n = x.shape[0]\n",
    "\n",
    "        \n",
    "        lmbs = np.random.dirichlet([self.alpha] * self.num_mix)\n",
    "\n",
    "        x_aug = lmbs[0] * x\n",
    "        y_aug = lmbs[0] * y_one_hot\n",
    "        for lmb in lmbs[1:]:\n",
    "           \n",
    "            indices = np.random.permutation(n)\n",
    "            x_aug = x_aug + lmb * x[indices]\n",
    "            y_aug = y_aug + lmb * y_one_hot[indices]\n",
    "\n",
    "        return x_aug, y_aug\n",
    "\n",
    "    def _check_params(self) -> None:\n",
    "        if self.num_classes <= 0:\n",
    "            raise ValueError(\"The number of classes must be positive\")\n",
    "\n",
    "        if self.alpha <= 0:\n",
    "            raise ValueError(\"The mixing interpolation strength must be positive.\")\n",
    "\n",
    "        if self.num_mix < 2:\n",
    "            raise ValueError(\"The number of samples to mix must be at least 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.defences.trainer.trainer import Trainer\n",
    "from typing import List, Optional, Union, Tuple, TYPE_CHECKING\n",
    "from tqdm.auto import trange\n",
    "import time\n",
    "if sys.version_info >= (3, 8):\n",
    "    from typing import Literal\n",
    "else:\n",
    "    from typing_extensions import Literal\n",
    "class DPInstaHideTrainer(Trainer):\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier: \"CLASSIFIER_LOSS_GRADIENTS_TYPE\",\n",
    "        augmentations: Union[\"Preprocessor\", List[\"Preprocessor\"]],\n",
    "        noise: Literal[\"gaussian\", \"laplacian\", \"exponential\"] = \"laplacian\",\n",
    "        loc: Union[int, float] = 0.0,\n",
    "        scale: Union[int, float] = 0.03,\n",
    "        clip_values: \"CLIP_VALUES_TYPE\" = (0.0, 1.0),\n",
    "    ):\n",
    "\n",
    "        from art.defences.preprocessor import Preprocessor\n",
    "\n",
    "        super().__init__(classifier)\n",
    "        if isinstance(augmentations, Preprocessor):\n",
    "            self.augmentations = [augmentations]\n",
    "        else:\n",
    "            self.augmentations = augmentations\n",
    "        self.noise = noise\n",
    "        self.loc = loc\n",
    "        self.scale = scale\n",
    "        self.clip_values = clip_values\n",
    "\n",
    "    def _generate_noise(self, x: np.ndarray) -> np.ndarray:\n",
    "        if self.noise == \"gaussian\":\n",
    "            noise = np.random.normal(loc=self.loc, scale=self.scale, size=x.shape)\n",
    "        elif self.noise == \"laplacian\":\n",
    "            noise = np.random.laplace(loc=self.loc, scale=self.scale, size=x.shape)\n",
    "        elif self.noise == \"exponential\":\n",
    "            noise = np.random.exponential(scale=self.scale, size=x.shape)\n",
    "        else:\n",
    "            raise ValueError(\"The provided noise type is not supported:\", self.noise)\n",
    "\n",
    "        x_noise = x + noise\n",
    "        x_noise = np.clip(x_noise, self.clip_values[0], self.clip_values[1])\n",
    "\n",
    "        return x_noise.astype(x.dtype)\n",
    "\n",
    "    def fit(  # pylint: disable=W0221\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        validation_data: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n",
    "        batch_size: int = 128,\n",
    "        nb_epochs: int = 20,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \n",
    "        logger.info(\"Performing adversarial training with DP-InstaHide protocol\")\n",
    "\n",
    "        nb_batches = int(np.ceil(len(x) / batch_size))\n",
    "        ind = np.arange(len(x))\n",
    "\n",
    "        logger.info(\"Adversarial Training DP-InstaHide\")\n",
    "\n",
    "        for i_epoch in trange(nb_epochs, desc=\"DP-InstaHide training epochs\"):\n",
    "           \n",
    "            np.random.shuffle(ind)\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "            train_n = 0.0\n",
    "\n",
    "            for batch_id in range(nb_batches):\n",
    "                \n",
    "                x_batch = x[ind[batch_id * batch_size : min((batch_id + 1) * batch_size, x.shape[0])]]\n",
    "                y_batch = y[ind[batch_id * batch_size : min((batch_id + 1) * batch_size, x.shape[0])]]\n",
    "\n",
    "               \n",
    "                x_aug = x_batch.copy()\n",
    "                y_aug = y_batch.copy()\n",
    "                for augmentation in self.augmentations:\n",
    "                    x_aug, y_aug = augmentation(x_aug, y_aug)\n",
    "\n",
    "               \n",
    "                x_aug = self._generate_noise(x_aug)\n",
    "\n",
    "                \n",
    "                self._classifier.fit(x_aug, y_aug, nb_epochs=1, batch_size=x_aug.shape[0], verbose=0, **kwargs)\n",
    "\n",
    "                \n",
    "                loss = self._classifier.compute_loss(x_aug, y_aug, reduction=\"mean\")\n",
    "                output = np.argmax(self.predict(x_batch), axis=1)\n",
    "                acc = np.sum(output == np.argmax(y_batch, axis=1))\n",
    "                n = len(x_aug)\n",
    "\n",
    "                \n",
    "                train_loss += np.sum(loss)\n",
    "                train_acc += acc\n",
    "                train_n += n\n",
    "\n",
    "            train_time = time.time()\n",
    "\n",
    "            \n",
    "            if validation_data is not None:\n",
    "                (x_test, y_test) = validation_data\n",
    "                output = np.argmax(self.predict(x_test), axis=1)\n",
    "                test_loss = self._classifier.compute_loss(x_test, y_test, reduction=\"mean\")\n",
    "                test_acc = np.mean(output == np.argmax(y_test, axis=1))\n",
    "                logger.info(\n",
    "                    \"epoch: %s time(s): %.1f, loss(tr): %.4f, acc(tr): %.4f, loss(val): %.4f, acc(val): %.4f\",\n",
    "                    i_epoch,\n",
    "                    train_time - start_time,\n",
    "                    train_loss / train_n,\n",
    "                    train_acc / train_n,\n",
    "                    test_loss,\n",
    "                    test_acc,\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    \"epoch: %s time(s): %.1f, loss: %.4f, acc: %.4f\",\n",
    "                    i_epoch,\n",
    "                    train_time - start_time,\n",
    "                    train_loss / train_n,\n",
    "                    train_acc / train_n,\n",
    "                )\n",
    "\n",
    "    def fit_generator(self, generator: \"DataGenerator\", nb_epochs: int = 20, **kwargs):\n",
    "\n",
    "        logger.info(\"Performing adversarial training with DP-InstaHide protocol\")\n",
    "        size = generator.size\n",
    "        batch_size = generator.batch_size\n",
    "        if size is not None:\n",
    "            nb_batches = int(np.ceil(size / batch_size))\n",
    "        else:\n",
    "            raise ValueError(\"Size is None.\")\n",
    "\n",
    "        logger.info(\"Adversarial Training DP-InstaHide\")\n",
    "\n",
    "        for i_epoch in trange(nb_epochs, desc=\"DP-InstaHide training epochs\"):\n",
    "            start_time = time.time()\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "            train_n = 0.0\n",
    "\n",
    "            for _ in range(nb_batches):\n",
    "               \n",
    "                x_batch, y_batch = generator.get_batch()\n",
    "\n",
    "                \n",
    "                x_aug = x_batch.copy()\n",
    "                y_aug = y_batch.copy()\n",
    "                for augmentation in self.augmentations:\n",
    "                    x_aug, y_aug = augmentation(x_aug, y_aug)\n",
    "\n",
    "                \n",
    "                x_aug = self._generate_noise(x_aug)\n",
    "\n",
    "                \n",
    "                self._classifier.fit(x_aug, y_aug, nb_epochs=1, batch_size=x_aug.shape[0], verbose=0, **kwargs)\n",
    "\n",
    "                \n",
    "                loss = self._classifier.compute_loss(x_aug, y_aug, reduction=\"mean\")\n",
    "                output = np.argmax(self.predict(x_batch), axis=1)\n",
    "                acc = np.sum(output == np.argmax(y_batch, axis=1))\n",
    "                n = len(x_aug)\n",
    "\n",
    "                \n",
    "                train_loss += np.sum(loss)\n",
    "                train_acc += acc\n",
    "                train_n += n\n",
    "\n",
    "            train_time = time.time()\n",
    "\n",
    "            \n",
    "            logger.info(\n",
    "                \"epoch: %s time(s): %.1f, loss: %.4f, acc: %.4f\",\n",
    "                i_epoch,\n",
    "                train_time - start_time,\n",
    "                train_loss / train_n,\n",
    "                train_acc / train_n,\n",
    "            )\n",
    "\n",
    "    def predict(self, x: np.ndarray, **kwargs) -> np.ndarray:\n",
    "        \n",
    "        return self._classifier.predict(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Performing adversarial training with DP-InstaHide protocol\n",
      "[INFO] Performing adversarial training with DP-InstaHide protocol\n",
      "INFO:root:Adversarial Training DP-InstaHide\n",
      "[INFO] Adversarial Training DP-InstaHide\n",
      "DP-InstaHide training epochs:   0%|          | 0/5 [00:00<?, ?it/s]INFO:root:epoch: 0 time(s): 26.2, loss: 0.0152, acc: 0.4559\n",
      "[INFO] epoch: 0 time(s): 26.2, loss: 0.0152, acc: 0.4559\n",
      "DP-InstaHide training epochs:  20%|██        | 1/5 [00:26<01:44, 26.19s/it]INFO:root:epoch: 1 time(s): 28.4, loss: 0.0126, acc: 0.5899\n",
      "[INFO] epoch: 1 time(s): 28.4, loss: 0.0126, acc: 0.5899\n",
      "DP-InstaHide training epochs:  40%|████      | 2/5 [00:54<01:22, 27.49s/it]INFO:root:epoch: 2 time(s): 31.9, loss: 0.0119, acc: 0.6230\n",
      "[INFO] epoch: 2 time(s): 31.9, loss: 0.0119, acc: 0.6230\n",
      "DP-InstaHide training epochs:  60%|██████    | 3/5 [01:26<00:59, 29.51s/it]INFO:root:epoch: 3 time(s): 36.1, loss: 0.0110, acc: 0.6654\n",
      "[INFO] epoch: 3 time(s): 36.1, loss: 0.0110, acc: 0.6654\n",
      "DP-InstaHide training epochs:  80%|████████  | 4/5 [02:02<00:32, 32.13s/it]INFO:root:epoch: 4 time(s): 39.6, loss: 0.0104, acc: 0.8126\n",
      "[INFO] epoch: 4 time(s): 39.6, loss: 0.0104, acc: 0.8126\n",
      "DP-InstaHide training epochs: 100%|██████████| 5/5 [02:42<00:00, 32.46s/it]\n"
     ]
    }
   ],
   "source": [
    "mixup = Mixup(num_classes=10, num_mix=2)\n",
    "trainer = DPInstaHideTrainer(\n",
    "    classifier=classifier,\n",
    "    augmentations=mixup,\n",
    "    noise='laplacian',\n",
    "    scale=0.3,\n",
    "    clip_values=(0, 1) \n",
    ")\n",
    "trainer.fit(x_train, y_train, nb_epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clean test set accuracy: 94.91%\n"
     ]
    }
   ],
   "source": [
    "clean_x_test = x_test[is_poison_test == 0]\n",
    "clean_y_test = y_test[is_poison_test == 0]\n",
    "\n",
    "clean_preds = np.argmax(classifier.predict(clean_x_test), axis=1)\n",
    "clean_correct = np.sum(clean_preds == np.argmax(clean_y_test, axis=1))\n",
    "clean_total = clean_y_test.shape[0]\n",
    "\n",
    "clean_acc = clean_correct / clean_total\n",
    "print(\"\\nClean test set accuracy: %.2f%%\" % (clean_acc * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Effectiveness of poison: 75.70%\n"
     ]
    }
   ],
   "source": [
    "poison_x_test = x_test[is_poison_test]\n",
    "poison_y_test = y_test[is_poison_test]\n",
    "\n",
    "poison_preds = np.argmax(classifier.predict(poison_x_test), axis=1)\n",
    "poison_correct = np.sum(poison_preds == np.argmax(poison_y_test, axis=1))\n",
    "poison_total = poison_y_test.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "poison_acc = poison_correct / poison_total\n",
    "print(\"\\n Effectiveness of poison: %.2f%%\" % (poison_acc * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp2",
   "language": "python",
   "name": "exp2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "30b15819a73bf738d2c051012d518af2175fe5da693b3ec4b95bab668851eb25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
