# Best_Practice_for_Security_of_AI
**There are related materials and codes in the folders named after each attack type**  
**Some classic reading materials are listed below**
## model inversion attack
[Privacy in Pharmacogenetics: An End-to-End Case Study of Personalized Warfarin Dosing](https://www.usenix.org/system/files/conference/usenixsecurity14/sec14-paper-fredrikson-privacy.pdf)  

[Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures](https://www.cs.cmu.edu/~mfredrik/papers/fjr2015ccs.pdf). 

[Membership Model Inversion Attacks for Deep Networks](https://arxiv.org/abs/1910.04257). 

[The Secret Revealer: Generative Model-Inversion Attacks Against Deep Neural Networks](https://arxiv.org/abs/1911.07135). 

## model extraction attack
# other resources for run
[ART](https://github.com/Trusted-AI/adversarial-robustness-toolbox)  
[advertorch](https://github.com/BorealisAI/advertorch)  
[CleverHans](https://github.com/cleverhans-lab/cleverhans/)  
[obfuscated-gradients](https://github.com/anishathalye/obfuscated-gradients)  
[adversarial-attacks-pytorch](https://github.com/Harry24k/adversarial-attacks-pytorch)
[robust-ml](https://www.robust-ml.org)
